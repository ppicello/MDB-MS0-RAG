from pymongo import MongoClient
from langchain_openai import OpenAIEmbeddings
from langchain_mongodb import MongoDBAtlasVectorSearch
from langchain_openai import OpenAI
from langchain.chains import RetrievalQA
from langchain.chains.prompt_selector import ConditionalPromptSelector, is_chat_model
from langchain_core.prompts import PromptTemplate

import gradio as gr
from gradio.themes.base import Base
import key_param


client = MongoClient(key_param.MONGO_URI)
dbName = key_param.MONGO_DB
collectionName = key_param.MONGO_COLL
collection = client[dbName][collectionName]

# Define the text embedding model
 
embeddings = OpenAIEmbeddings(openai_api_key=key_param.openai_api_key, model="text-embedding-ada-002")

# Initialize the Vector Store
index_name = "vector_index"
vector_field_name = "embedding"

vectorStore = MongoDBAtlasVectorSearch(collection, embeddings,index_name=index_name, embedding_key=vector_field_name, )

def query_data(query):
    # Convert question to vector using OpenAI embeddings
    # Perform Atlas Vector Search using Langchain's vectorStore
    # similarity_search returns MongoDB documents most similar to the query    

    docs = vectorStore.similarity_search(query, K=1)
    as_output = docs[0].page_content

    print(as_output)

    # Leveraging Atlas Vector Search paired with Langchain's QARetriever

    # Define the LLM that we want to use -- note that this is the Language Generation Model and NOT an Embedding Model
    # If it's not specified (for example like in the code below),
    # then the default OpenAI model used in LangChain is OpenAI GPT-3.5-turbo, as of August 30, 2023
    
    llm = OpenAI(model_name="gpt-3.5-turbo-instruct", openai_api_key=key_param.openai_api_key, temperature=0)


    # Get VectorStoreRetriever: Specifically, Retriever for MongoDB VectorStore.
    # Implements _get_relevant_documents which retrieves documents relevant to a query.
    retriever = vectorStore.as_retriever()

    # Load "stuff" documents chain. Stuff documents chain takes a list of documents,
    # inserts them all into a prompt and passes that prompt to an LLM.

    qa = RetrievalQA.from_chain_type(llm, chain_type="stuff", retriever=retriever)

    # Execute the chain
    retriever_output = qa.run(query)

    # Return Atlas Vector Search output, and output generated using RAG Architecture
    return as_output, retriever_output

def query_data_custom_prompt(query):
    docs = vectorStore.similarity_search(query, K=1)
    as_output = docs[0].page_content
    llm = OpenAI(model_name="gpt-3.5-turbo-instruct", openai_api_key=key_param.openai_api_key, temperature=0)

    retriever = vectorStore.as_retriever()

    prompt_template = """

      I want you to act as a rapper.  You have to give the answer as it was an american rap song, everything should be in rhyme.

      Use the following pieces of context to answer the question at the end.If you don't know the answer, just say that you don't know, don't try to make up an answer.

      {context}

      Question: {question}

      Helpful Answer:"""

    PROMPT = PromptTemplate(
        template=prompt_template, input_variables=["context", "question"]
    )

    chain_type_kwargs = {"prompt": PROMPT}

    qa = RetrievalQA.from_chain_type(llm, chain_type="stuff", retriever=retriever, chain_type_kwargs=chain_type_kwargs)
    retriever_output = qa.run(query)
    return as_output, retriever_output

# Create a web interface for the app, using Gradio
with gr.Blocks(theme=Base(), title="Question Answering App using Vector Search + RAG") as demo:
    gr.Markdown(
        """
        # Question Answering App using Atlas Vector Search + RAG Architecture
        """)
    textbox = gr.Textbox(label="Enter your Question:")
    with gr.Row():
        button = gr.Button("Submit", variant="primary")
    with gr.Column():
        output1 = gr.Textbox(lines=1, max_lines=10, label="Output with just Atlas Vector Search (returns text field as is):")
        output2 = gr.Textbox(lines=1, max_lines=10, label="Output generated by chaining Atlas Vector Search to Langchain's RetrieverQA + OpenAI LLM:")

# Call query_data function upon clicking the Submit button

    button.click(query_data, textbox, outputs=[output1, output2])

demo.launch()
